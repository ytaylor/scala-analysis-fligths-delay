# scala-analysis-fligths-delay

This project use vuelos dataset fo perform predictive models using supervised learning algorithms and validate them cross way,  using Spark MLLib. This projetc include
* Definition of some Scala functions for preprocessing.
* Modeled with Spark and MLLib using Decision Trees and Naive Bayes.
* Build an interactive and predictive model for flight delays

## Preparation of data.
For the preparation of the data, we carry out cleaning, transformation, and main feature selection. For the analyzed data set the class does not finds explicit, so we have used our own function parseData () that is responsible for creating the delay class, we consider the flight delays of 40 minutes or more like delays and we mark it with a label of 1.0 and less than 40 minutes as not delay and we mark it with a label of 0.0.

We use Spark's RDDs to perform the same previous processing, transforming the raw flight delay data set in two feature matrices 
trainingData the training set and testData the test set. Further, we use the RDD cache method to ensure that these calculated RDDs (trainingData, testData) are cached in memory by Spark and not recalculated with each iteration.

With the data set trainingData (which we will use for training) and the set of data testData (which we will use for validation) as RDD, now we will build a predictive model using the Spark MLLib learning library.

## Creation and selection of models.
With basic characteristics of the data set and created the RDD, we proceed to the creation of the models. In order to compare the performance and the use of different models, the model is trained using Naïve Bayes and Decision Tree. 

### Cross validation

Cross validation is a critical part of the real world learning machine and it is fundamental for the selection of many models and the adjustment of parameters. The idea general behind the cross validation is that we want to know how our model is
will perform on unseen data. Cross-validation provides a mechanism in which we use part of our data set available to train our model and another part to evaluate the performance of this model.

As the model is tested with data that you have not seen during the training phase, when it is evaluated in this part of the data set, it gives us how well our model

It is generalized for the new data points.To carry out the cross-validation we have helped the MLUTils library of
MLIB, which has a kFold function that given the data set and number of iterations k performs partitions to the data set provided in sets of training and tests.

### Decision Tree
The first step is to determine the depth of the tree so that it is the best
possible, for this we have used cross-validation, which allows us to decide what
parameters are the most appropriate. The parameter of maximum depth of the tree controls
through the depth of the tree the complexity of the model. The deepest trees
will be able to better adapt the data. Also for this kind of problems of
classification, you can select between two measures of impurities: Gini and Entropy.

### Naive Bayes 

For the construction of the Naive Bayes classifier, we have chosen the multinomial type and the
same way that with the previous classifier we look for the suitable parameters by the
algorithm. In this case the parameter to be determined is the lambda, which is not very
determinant which we will see below.

## Evaluation of the models in the test set.

When we make predictions using our model, as has been done in the
previous sections, how do we know if the predictions are good or not?
able to evaluate if the model works. The evaluation metrics commonly
used in the classification are: precision, recall, curve for Receiver Operation

### Error rate, its standard deviation and its confidence interval for a confidence 95%
The error rate is the proportion of the number of errors committed on a set of
instances. To calculate the deviation of the error, we used the formula that
It is shown below. It is only represented for the Naive Bayes classifier, since it has been
followed the same sequence of steps for the Decision Tree classifier.

```
/*Tasa de error, su desviación estándar y su intervalo de confianza para una confianza del 95%.*/
val errores_nb = predictionsAndLabels_nb.map(x => if (x._1 == x._2) 0 else 1).sum
val tasaError_nb= errores_nb/predictionsAndLabels_nb.count
val desvStandr_nb = Math.sqrt(tasaError_nb*(1-tasaError_nb)/predictionsAndLabels_nb.count)

val intervaloconfianza_positive_nb= tasaError_nb + 2.4729 * desvStandr_nb
val intervaloconfianza_negative_nb= tasaError_nb - 2.4729 * desvStandr_nb
```

### Rate of certain positives and Rate of false positives.
In a supervised classification problem, there is a real output and a predicted result generated by the model for each data point. For this reason, the results of each

Data point can be assigned to one of four categories:
* True Positive (TP) - the label is positive and the prediction is also positive
* True Negative (TN) - the label is negative and the prediction is also negative
* False Positive (FP) - the label is negative but the prediction is positive
* False Negative (FN) - the label is positive, but the prediction is negative

These four numbers are the building blocks for most of the metrics of
evaluation of classifiers. A fundamental point when considering the evaluation of
classifier is that pure precision (that is, correct or incorrect prediction) is not
generally a good metric.

The reason for this is because a data set can be highly unbalanced. By
example, if a model is designed to predict the fraud of a data set where
95% of the data points are not frauds and 5% of the data points are
fraudulent, then it is a naive classifier that does not predict fraud.

### Accuracy
The meaning of this evaluation metric suppose a model predicts that 8 of these are true delays, accuracy is
precision is not a good metric, the reason unbalanced we will make it through an example. every 1000 flights are delayed, if only 6 of 6/8 = 0.75. In general, it is considered that is that the data can be highly.

### Recall
The meaning of this evaluation metric will be based on the example previously, we knew that 6 out of every 1000 flights were really a delay. The model predicted correctly 5 of them. He also predicted 3 incorrectly, the memory therefore 5/6 =
0.833

### Area under the ROC curve.
The ROC curve presents in a visual way the ratio of the positive true (TPR) and the ratio of the false ones
positive (FPR). The ratio is calculated from the following equations:

```
TPR = true positives /true positives + false negatives
TFR = false positives /false positives + true negatives
```

Each point of the curve has a different threshold depending on the decision classification. Area low ROC is called AUC and represents the mean value, ie 1.0 means a perfect classifier and 0.5 refers to score which is the same as random guessing.

### Area under the PR curve.
Low area of PR refers to the mean of the precision value, that is, if it takes value 1.0 on the Classifier is perfect and has 100% accuracy in accuracy and Recall.
